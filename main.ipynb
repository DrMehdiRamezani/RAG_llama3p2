{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain langchain_community tiktoken langchain-nomic \"nomic[local]\" langchain-ollama scikit-learn langgraph tavily-python bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-nomic in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (0.1.3)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.2.40 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-nomic) (0.3.12)\n",
      "Requirement already satisfied: nomic<4.0.0,>=3.1.2 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-nomic) (3.1.2)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-nomic) (10.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.2.40->langchain-nomic) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.2.40->langchain-nomic) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.2.40->langchain-nomic) (0.1.136)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.2.40->langchain-nomic) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.2.40->langchain-nomic) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.2.40->langchain-nomic) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langchain-core<0.4,>=0.2.40->langchain-nomic) (4.12.2)\n",
      "Requirement already satisfied: click in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (8.1.7)\n",
      "Requirement already satisfied: jsonlines in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (4.0.0)\n",
      "Requirement already satisfied: loguru in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (0.7.2)\n",
      "Requirement already satisfied: rich in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (13.9.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (2.32.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (2.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (4.66.5)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (17.0.0)\n",
      "Requirement already satisfied: pyjwt in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (2.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.40->langchain-nomic) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.40->langchain-nomic) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.40->langchain-nomic) (3.10.9)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.40->langchain-nomic) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.40->langchain-nomic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.40->langchain-nomic) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from requests->nomic<4.0.0,>=3.1.2->langchain-nomic) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from requests->nomic<4.0.0,>=3.1.2->langchain-nomic) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from requests->nomic<4.0.0,>=3.1.2->langchain-nomic) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from requests->nomic<4.0.0,>=3.1.2->langchain-nomic) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from click->nomic<4.0.0,>=3.1.2->langchain-nomic) (0.4.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from jsonlines->nomic<4.0.0,>=3.1.2->langchain-nomic) (24.2.0)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from loguru->nomic<4.0.0,>=3.1.2->langchain-nomic) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from rich->nomic<4.0.0,>=3.1.2->langchain-nomic) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from rich->nomic<4.0.0,>=3.1.2->langchain-nomic) (2.18.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.40->langchain-nomic) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.40->langchain-nomic) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.40->langchain-nomic) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.40->langchain-nomic) (0.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->nomic<4.0.0,>=3.1.2->langchain-nomic) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rit\\documents\\dev\\rag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-nomic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama pull llama3.2:3b-instruct-fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "# local_llm = \"llama3.2:1b-instruct-fp16\"\n",
    "local_llm = \"llama3.2:1b\"\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import getpass\n",
    "\n",
    "\n",
    "# def _set_env(var: str):\n",
    "#     if not os.environ.get(var):\n",
    "#         os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# _set_env(\"TAVILY_API_KEY\")\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# _set_env(\"LANGCHAIN_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"local-llama32-rag\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Database , URL loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "import os\n",
    "cwd_path= os.getcwd()\n",
    "VectorStore_path=os.path.join(cwd_path,\"VectorStore\",\"union.parquet\")\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "    # ,\n",
    "    # \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    # \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "# Load documents\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "    persist_path= VectorStore_path,\n",
    "    serializer=\"parquet\"\n",
    ")\n",
    "\n",
    "# # save the vectorestore\n",
    "# VectorStore.persist()\n",
    "# print(\"Vector store was persisted to\", VectorStore_path)\n",
    "\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Powered Autonomous Agents | Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lil'Log\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Posts\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Archive\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tags\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FAQ\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "emojisearch.app\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "\n",
      "\n",
      "Agent System Overview\n",
      "\n",
      "Component One: Planning\n",
      "\n",
      "Task Decomposition\n",
      "\n",
      "Self-Reflection\n",
      "\n",
      "\n",
      "Component Two: Memory\n",
      "\n",
      "Types of Memory\n",
      "\n",
      "Maximum Inner Product Search (MIPS)\n",
      "\n",
      "\n",
      "Component Three: Tool Use\n",
      "\n",
      "Case Studies\n",
      "\n",
      "Scientific Discovery Agent\n",
      "\n",
      "Generative Agents Simulation\n",
      "\n",
      "Proof-of-Concept Examples\n",
      "\n",
      "\n",
      "Challenges\n",
      "\n",
      "Citation\n",
      "\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
      "\n",
      "Planning\n",
      "\n",
      "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
      "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
      "\n",
      "\n",
      "Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\n",
      "\n",
      "\n",
      "Tool use\n",
      "\n",
      "The agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "import os\n",
    "cwd_path= os.getcwd()\n",
    "VectorStore_path=os.path.join(cwd_path,\"VectorStore\",\"union.parquet\")\n",
    "\n",
    "# # save the vectorestore\n",
    "# VectorStore.persist()\n",
    "# print(\"Vector store was persisted to\", VectorStore_path)\n",
    "\n",
    "# load the vectorestore\n",
    "vectorstore_load= SKLearnVectorStore(\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "    persist_path= VectorStore_path,\n",
    "    serializer=\"parquet\"\n",
    ")\n",
    "\n",
    "## query\n",
    "query= \"agent\"\n",
    "documents = vectorstore_load.similarity_search(query)\n",
    "print(documents[0].page_content)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore_load.as_retriever(k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Reflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\n",
      "\n",
      "Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\n",
      "\n",
      "\n",
      "Planning & Reacting: translate the reflections and the environment information into actions\n",
      "\n",
      "Planning is essentially in order to optimize believability at the moment vs in time.\n",
      "Prompt template: {Intro of an agent X}. Here is X's plan today in broad strokes: 1)\n",
      "Relationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\n",
      "Environment information is present in a tree structure.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fig. 13. The generative agent architecture. (Image source: Park et al. 2023)\n",
      "This fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\n",
      "Proof-of-Concept Examples#\n",
      "AutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\n",
      "Here is the system message used by AutoGPT, where {{...}} are user inputs:\n",
      "You are {{ai-name}}, {{user-provided AI bot description}}.\n",
      "Your decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\n",
      "\n",
      "GOALS:\n",
      "\n",
      "1. {{user-provided goal 1}}\n",
      "2. {{user-provided goal 2}}\n",
      "3. ...\n",
      "4. ...\n",
      "5. ...\n",
      "\n",
      "Constraints:\n",
      "1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\n",
      "2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\n",
      "3. No user assistance\n",
      "4. Exclusively use the commands listed in double quotes e.g. \"command name\"\n",
      "5. Use subprocesses for commands that will not terminate within a few minutes' metadata={'id': 'dbed19f6-7da1-4adb-a759-a30148a7f423', 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}\n"
     ]
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Database , pdf Loader from Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-5.0.1-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store was persisted to c:\\Users\\RIT\\Documents\\dev\\RAG\\VectorStore\\pdf_VectorStore.parquet\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader #for pdf\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "import os\n",
    "cwd_path= os.getcwd()\n",
    "VectorStore_path=os.path.join(cwd_path,\"VectorStore\",\"pdf_VectorStore.parquet\")\n",
    "Inputs_path = os.path.join(cwd_path,\"Inputs\")\n",
    "def list_pdf_files(directory):\n",
    "    return [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.pdf')]\n",
    "\n",
    "\n",
    "# Print the path directory to all of the .pdf files\n",
    "# print(Inputs_path)\n",
    "\n",
    "# List comprehension to get all .pdf files in the directory\n",
    "# pdf_files = [file for file in os.listdir(Inputs_path) if file.endswith('.pdf')]\n",
    "pdf_files = list_pdf_files(Inputs_path)\n",
    "# print(pdf_files)\n",
    "\n",
    "pdf_docs= [PyPDFLoader(pdf_file, extract_images= False).load() for pdf_file in pdf_files]\n",
    "pdf_docs_list = [item for sublist in pdf_docs for item in sublist]\n",
    "\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "pdf_doc_splits = text_splitter.split_documents(pdf_docs_list)\n",
    "\n",
    "\n",
    "# Add to vectorDB\n",
    "pdf_vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=pdf_doc_splits,\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "    persist_path= VectorStore_path,\n",
    "    serializer=\"parquet\"\n",
    ")\n",
    "\n",
    "# save the vectore store \n",
    "pdf_vectorstore.persist()\n",
    "print(\"Vector store was persisted to\", VectorStore_path)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "# local_llm = \"llama3.2:1b-instruct-fp16\"\n",
    "local_llm = \"llama3.2:1b\"\n",
    "\n",
    "import os\n",
    "# _set_env(\"LANGCHAIN_API_KEY\")\n",
    "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"local-llama32-rag\"\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader #for pdf\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "import os\n",
    "cwd_path= os.getcwd()\n",
    "VectorStore_path=os.path.join(cwd_path,\"VectorStore\",\"pdf_VectorStore.parquet\")\n",
    "Inputs_path = os.path.join(cwd_path,\"Inputs\")\n",
    "def list_pdf_files(directory):\n",
    "    return [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.pdf')]\n",
    "\n",
    "# load the vectorestore\n",
    "vectorstore_load= SKLearnVectorStore(\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "    persist_path= VectorStore_path,\n",
    "    serializer=\"parquet\"\n",
    ")\n",
    "\n",
    "# ## query\n",
    "# query= \"agent\"\n",
    "# documents = vectorstore_load.similarity_search(query)\n",
    "# print(documents[0].page_content)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore_load.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain of Thought (CoT) prompting refers to a process where an external tool or planning system takes over the planning step, allowing autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. This is achieved through techniques such as self-reflection, reasoning, and acting within Large Language Models (LLMs). The goal is to enable LLMs to generate outputs with incremental improvement in a sequence, similar to how humans learn from feedback sequences.\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "### Retrieval Grader\n",
    "\n",
    "# Doc grader instructions\n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "# Test\n",
    "question = \"What is Chain of thought prompting?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "    document=doc_txt, question=question\n",
    ")\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=doc_grader_instructions)]\n",
    "    + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    ")\n",
    "json.loads(result.content)\n",
    "\n",
    "\n",
    "### Generate\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Test\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datasource': 'vectorstore', 'data': [{'id': 1, 'name': 'Recurrent Neural Network (RNN)', 'description': 'A type of neural network that uses recurrent connections to process sequential data.'}, {'id': 2, 'name': 'Convolutional Neural Network (CNN)', 'description': 'A type of neural network that uses convolutional and pooling layers to process images.'}, {'id': 3, 'name': 'Transformer', 'description': 'A type of neural network that uses self-attention mechanisms to process sequential data.'}]}\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "import json\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Prompt\n",
    "router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "\n",
    "Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.\n",
    "\n",
    "Return JSON with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question.\"\"\"\n",
    "\n",
    "# Test router\n",
    "# test_web_search = llm_json_mode.invoke(\n",
    "#     [SystemMessage(content=router_instructions)]\n",
    "#     + [\n",
    "#         HumanMessage(\n",
    "#             content=\"Who is favored to win the NFC Championship game in the 2024 season?\"\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "# test_web_search_2 = llm_json_mode.invoke(\n",
    "#     [SystemMessage(content=router_instructions)]\n",
    "#     + [HumanMessage(content=\"What are the models released today for llama3.2?\")]\n",
    "# )\n",
    "test_vector_store = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=router_instructions)]\n",
    "    + [HumanMessage(content=\"What are the types of agent memory?\")]\n",
    ")\n",
    "print(\n",
    "    # json.loads(test_web_search.content),\n",
    "    # json.loads(test_web_search_2.content),\n",
    "    json.loads(test_vector_store.content),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "# Doc grader instructions\n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "\n",
    "# Test\n",
    "question = \"What is Chain of thought prompting?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "    document=doc_txt, question=question\n",
    ")\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=doc_grader_instructions)]\n",
    "    + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    ")\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain of Thought (CoT) prompting is a technique used in natural language processing (NLP) where LLMs are trained to follow instructions that mimic human thought processes, such as breaking down complex tasks into smaller steps and generating reasoning traces in natural language. This approach enables LLMs to interact with the environment (e.g., use Wikipedia search API), while also prompting them to generate outputs with incremental improvement in a sequence. By distilling learning histories from multiple episodes, CoT can learn to take on trends and improve performance over time.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Test\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "print(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes',\n",
       " 'explanation': 'The paper discusses Algorithm Distillation (AD), which applies the idea of sequentially improved outputs in context to cross-episode trajectories in reinforcement learning tasks. It hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# Hallucination grader instructions\n",
    "hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "# Test using documents and generation from above\n",
    "hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
    "    documents=docs_txt, generation=generation.content\n",
    ")\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=hallucination_grader_instructions)]\n",
    "    + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
    ")\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'binary_score': 'yes',\n",
       " 'explanation': \"The student's answer provides additional information about the vision models released today as part of Llama 3.2, specifically mentioning two models (Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct) that are available on Azure AI Model Catalog via managed compute. This meets the criteria because the answer contains extra information beyond what is explicitly asked for in the question.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# Answer grader instructions\n",
    "answer_grader_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "\n",
    "(1) The STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "Score:\n",
    "\n",
    "A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "The student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.\n",
    "\n",
    "A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "answer_grader_prompt = \"\"\"QUESTION: \\n\\n {question} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "# Test\n",
    "question = \"What are the vision models released today as part of Llama 3.2?\"\n",
    "answer = \"The Llama 3.2 models released today include two vision models: Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct, which are available on Azure AI Model Catalog via managed compute. These models are part of Meta's first foray into multimodal AI and rival closed models like Anthropic's Claude 3 Haiku and OpenAI's GPT-4o mini in visual reasoning. They replace the older text-only Llama 3.1 models.\"\n",
    "\n",
    "# Test using question and generation from above\n",
    "answer_grader_prompt_formatted = answer_grader_prompt.format(\n",
    "    question=question, generation=answer\n",
    ")\n",
    "result = llm_json_mode.invoke(\n",
    "    [SystemMessage(content=answer_grader_instructions)]\n",
    "    + [HumanMessage(content=answer_grader_prompt_formatted)]\n",
    ")\n",
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Search\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n",
    "    \"\"\"\n",
    "\n",
    "    question: str  # User question\n",
    "    generation: str  # LLM generation\n",
    "    web_search: str  # Binary decision to run web search\n",
    "    max_retries: int  # Max number of retries for answer generation\n",
    "    answers: int  # Number of answers generated\n",
    "    loop_step: Annotated[int, operator.add]\n",
    "    documents: List[str]  # List of retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema import Document\n",
    "# from langgraph.graph import END\n",
    "\n",
    "\n",
    "### Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Write retrieved documents to documents key in state\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "\n",
    "    # RAG generation\n",
    "    docs_txt = format_docs(documents)\n",
    "    rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"generation\": generation, \"loop_step\": loop_step + 1}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "            document=d.page_content, question=question\n",
    "        )\n",
    "        result = llm_json_mode.invoke(\n",
    "            [SystemMessage(content=doc_grader_instructions)]\n",
    "            + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "        )\n",
    "        grade = json.loads(result.content)[\"binary_score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    route_question = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=router_instructions)]\n",
    "        + [HumanMessage(content=state[\"question\"])]\n",
    "    )\n",
    "    source = json.loads(route_question.content)[\"datasource\"]\n",
    "    if source == \"websearch\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source == \"vectorstore\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: NOT ALL DOCUMENTS ARE RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    max_retries = state.get(\"max_retries\", 3)  # Default to 3 if not provided\n",
    "\n",
    "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
    "        documents=format_docs(documents), generation=generation.content\n",
    "    )\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=hallucination_grader_instructions)]\n",
    "        + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
    "    )\n",
    "    grade = json.loads(result.content)[\"binary_score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        # Test using question and generation from above\n",
    "        answer_grader_prompt_formatted = answer_grader_prompt.format(\n",
    "            question=question, generation=generation.content\n",
    "        )\n",
    "        result = llm_json_mode.invoke(\n",
    "            [SystemMessage(content=answer_grader_instructions)]\n",
    "            + [HumanMessage(content=answer_grader_prompt_formatted)]\n",
    "        )\n",
    "        grade = json.loads(result.content)[\"binary_score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        elif state[\"loop_step\"] <= max_retries:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "            return \"max retries\"\n",
    "    elif state[\"loop_step\"] <= max_retries:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "    else:\n",
    "        print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "        return \"max retries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generate\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"websearch\",\n",
    "        \"max retries\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"question\": \"What are the types of agent memory?\", \"max_retries\": 3}\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://smith.langchain.com/public/1e01baea-53e9-4341-a6d1-b1614a800a97/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on current events\n",
    "inputs = {\n",
    "    \"question\": \"What are the models released today for llama3.2?\",\n",
    "    \"max_retries\": 3,\n",
    "}\n",
    "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://smith.langchain.com/public/acdfa49d-aa11-48fb-9d9c-13a687ff311f/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
